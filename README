nivdror1, ido.shachar
niv dror (305498198), Ido Shachar (311248355)
EX: 4

FILES:
Block.cpp
Block.h
CacheAlgorithm.cpp
CacheAlgorithm.h
FBRAlgo.h
FBRAlgo.cpp
LRUAlgo.cpp
LRUAlgo.h
LFUAlgo.cpp
LFUAlgo.h
CacheFS.cpp
CacheFile.h
CacheFile.cpp
README
Makefile

REMARKS:
Brief description of our code:
In this exercise there are 2 major parts to take care of, the first part is the interface between
the library and the clients and the second part is managing the cache.
We've tried to separate those two part as much as we can, such that the job of the library is to
manage the files and their file descriptors and to copy the information from the cache to user's
buffer.
The job of the parent class CacheAlgorithm is to check if needed blocks are in the cache and read
information from them, where every subclass define his replacing policy in cases of cache hit or
miss.
The flow of the program is as follow:
Open files - we open the files according to absolute path, meaning that if the user try to open a
file that already open, we won't really open a new file. Instead, we return a fake file descriptor
and add a pair of (fakeFD, realFD) to a map, where fakeFD is a counter that we made and realFD is
the file descriptor that the open function return when we try to open this file for the first
(and only) time. Any time a new pair like this add to the map we increase the reference counter of
the CacheFile object that represent this file.
Close files - in any call for close file we decrease the reference counter of this file and erase
this (fakeFD, realFD) pair from the map. We actually close the file by his real file descriptor
only if the reference counter turn into zero.
Read from file - the library decide which block to read and how much to read from each block,
when the CacheAlgorithm actually supply the information from the block (he knows only blocks, not
 files) and update the cache properly.


ANSWERS:

part 1:
1) a) round robin:

the gantt chart:
0-2 - p1
2-4 - p2
4-6 - p1
6-7 - p2
7-9 - p3
9-11 - p4
11-13 - p1
13-14 - p5
14-16 - p4
16-18 - p1
18-20 - p4
20-22 - p1
22-23 - p4

turnaround time - 11.6 without cs

average wait time - 7 without cs

b) first come first serve:

the gantt chart:
0-10 - p1
10-13 - p2
13-15 - p3
15-22 - p4
22-23 - p1

turnaround time - 13.2 without cs

average wait time - 12 without cs

c) SRTF:

the gantt chart:

0-2 - p1
2-5 - p2
5-7 - p3
7-8 - p5
8-15 - p4
15-23 - p1

turnaround time - 8.2 without cs

average wait time - 3.6 without cs

d) priority scheduling

the gantt chart:

0-10 - p1
10-13 - p2
13-20 - p4
20-22 - p3
22-23 - p5

turnaround time - 14.2 without cs

average wait time - 13 without cs

e) priority scheduling with preemption:

the gantt chart:
0-2 - p1
2-5 - p2
5-12 - p4
12-14 - p3
14-22 - p1
22-23 - p5

turnaround time - 11.8 without cs

average wait time - 7.2 without cs

2)
Yes, it always provide a faster access time than fetching from the disk because the heap is
located at the main memory.
The main memory is DRAM and since accessing DRAM is much faster then accessing the disk (which
 must be accessed mechanically) we get that this way is better.

3)
Swapping pages algorithm is managed by the MMU/CPU, in hardware level, and since that it is very
difficult to run sophisticated algorithms that demands sophisticated data structures so it has to
use simple clock-oriented algorithms.
In contrast, managing disk buffer is os's responsibility so we have more possibilities in sense
of memory and sophisticated data structures.

4)
An example when LRU is better than LFU:

Let the size of the cache be 4.
Initially the user reads the blocks 0,1,2 for a 1000 times.
Then the user reads the block 3,4 for infinity.

An example when LFU is better than LRU:
Let the size of the cache be 3.
Reading order of the blocks is - 1,2,2,3,3,3,4,1,2

An example when LFU and LRU are both not good:
Let the size of the cache be 3.
Reading order of the blocks is - 1,2,3,4,5,6,1,2,3

5)

The reason for that behaviour is to "factoring out locality", that means that if we have a
block/file that is very necessary for short period of time, and for any time else we don't mean
to access it again. So with that kind of file, certain block will have very high number of
reference - so it get stuck in the cache for a long time, although no one will access it again.

part two:

1)
As we saw in class, every access to file demand access to the I-node and directory instructor of
every directory in the path. With that in mind, the following disk access needed:
1. Access to the i-node and instructor of the root directory ("\").
2. Access to the i-node and instructor of os directory.
3. Access to the i-node of "README" file.

Up until now we get 5 access to the disk, now we need to access the block with the single
indirect (6) (since we can't access directly after the 20,480 byte), find the pointer that points to
40000 byte, read block's data (7), write (8) the changes we want to do in that block and at last
update (9) the inode of this file.
Therefore, we get 9 access to the disk.

2)
First, we observe that seek and read are system calls, so we get 2 trap software interrupts.
Second, read function has to access the disk, so when the disk will finish his job it will send
memory hardware interrupt to notify the os that the job has done.
Overall we get 3 interrupts.

3)
a)
We will use two level feedback queue, where every new jobs that came to the system gets into the
first queue, that will run Round-Robin algorithm with quantum size=1. Each work that haven't
finished her job (has to be type 2 job, with running time of 10 seconds) gets into second queue
that will run SJF with priority and preemption for jobs that just came in to this queue.
This preemption and priority means that if a new job enters to the first queue, or moves from
first to second queue, we stop the job that running right now and runs this new job.

b)
No, the preemption will cause many more context switch than first come first serve algorithm, and
 more context switch will cause more overhead time.
 First come first serve will minimize overhead time because in that algorithm there are no
 context switch at all, hence the CPU always busy only with running process.