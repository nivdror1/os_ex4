nivdror1, ido.shachar
niv dror (305498198), Ido Shachar (311248355)
EX: 4

FILES:
Block.cpp
Block.h
CacheAlgorithm.cpp
CacheAlgorithm.h
FBRAlgo.h
FBRAlgo.cpp
LRUAlgo.cpp
LRUAlgo.h
LFUAlgo.cpp
LFUAlgo.h
CacheFS.cpp
CacheFile.h
CacheFile.cpp
README
Makefile

REMARKS:
Brief description of our code:
In this exercise there are 2 major parts to take care of, the first part is the interface between
the library and the clients and the second part is managing the cache.
We've tried to separate those two part as much as we can, such that the job of the library is to
manage the files and their file descriptors and to copy the information from the cache to user's
buffer.
The job of the parent class CacheAlgorithm is to check if needed blocks are in the cache and read
information from them, where every subclass define his replacing policy in cases of cache hit or
miss.
The flow of the program is as follow:
Open files - we open the files according to absolute path, meaning that if the user try to open a
file that already open, we won't really open a new file. Instead, we return a fake file descriptor
and add a pair of (fakeFD, realFD) to a map, where fakeFD is a counter that we made and realFD is
the file descriptor that the open function return when we try to open this file for the first
(and only) time. Any time a new pair like this add to the map we increase the reference counter of
the CacheFile object that represent this file.
Close files - in any call for close file we decrease the reference counter of this file and erase
this (fakeFD, realFD) pair from the map. We actually close the file by his real file descriptor
only if the reference counter turn into zero.
Read from file - the library decide which block to read and how much to read from each block,
when the CacheAlgorithm actually supply the information from the block (he knows only blocks, not
 files) and update the cache properly.


ANSWERS:

part 1:
1) a) round robin:

the gantt chart:
0-2 - p1
2-4 - p2
4-6 - p3
6-8 - p4
8-9 - p5
9-11 - p1
11-12 - p2
12-14 - p4
14-16 - p1
16-18 - p4
18-20 - p1
20-21 - p4
21-23 - p1

turnaround time - 10.8 without cs

average wait time - 9.6 without cs

b) first come first serve:

the gantt chart:
0-10 - p1
10-13 - p2
13-15 - p3
15-22 - p4
22-23 - p1

turnaround time - 13.2 without cs

average wait time - 12 without cs

c) SRTF:

the gantt chart:

0-2 - p1
2-5 - p2
5-7 - p3
7-8 - p5
8-15 - p4
15-23 - p1

turnaround time - 8.2 without cs

average wait time - 3.6 without cs

d) priority scheduling

the gantt chart:

0-10 - p1
10-13 - p2
13-20 - p4
20-22 - p3
22-23 - p5

turnaround time - 14.2 without cs

average wait time - 13 without cs

e) priority scheduling with preemption:

the gantt chart:
0-2 - p1
2-5 - p2
5-12 - p4
12-14 - p3
14-22 - p1
22-23 - p5

turnaround time - 11.8 without cs

average wait time - 7.2 without cs

2)
Yes, it always provide a faster access time than fetching from the disk because the heap is
located at the main memory.
The main memory is DRAM and therefore accessing DRAM is much faster then accessing the disk which
 must be accessed mechanically.

3)

4)
An example when LRU is better than LFU:

Let the size of the cache be 4.
Initially the user reads the blocks 0,1,2 for a 1000 times.
Then the user reads the block 3,4 for infinity.

An example when LFU is better than LRU:
Let the size of the cache be 3.
The reading of the block is - 1,2,2,3,3,3,4,1,2

An example when LFU and LRU are both not good:
Let the size of the cache be 3.
The reading of the block is - 1,2,3,4,5,6,1,2,3

5)

The reason for that behaviour is to "factoring out locality", that means that if we have a
block/file that is very necessary for short period of time, and for any time else we don't mean
to access it again. So with that kind of file, certain block will have very high number of
reference - so it get stuck in the cache for a long time, although no one will access it again.

